# Test Design: Story 1.3 - Consistency Guardian Agent

Date: 2025-08-31
Designer: Quinn (Test Architect)

## Test Strategy Overview

- Total test scenarios: 42
- Unit tests: 22 (52%)
- Integration tests: 14 (33%)
- E2E tests: 6 (15%)
- Priority distribution: P0: 18, P1: 15, P2: 9

## Test Scenarios by Acceptance Criteria

### AC1: Agent能够接收结构化的剧本数据进行分析

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.3-UNIT-001 | Unit | P0 | Accept valid ParsedScript interface | Core functionality |
| 1.3-UNIT-002 | Unit | P0 | Validate script data structure | Input validation |
| 1.3-UNIT-003 | Unit | P1 | Handle empty script gracefully | Edge case |
| 1.3-INT-001 | Integration | P0 | Process parser output directly | Integration point |
| 1.3-INT-002 | Integration | P1 | Handle malformed script data | Error handling |
| 1.3-E2E-001 | E2E | P1 | Complete script analysis flow | User journey |

### AC2: 能够检测至少5种核心逻辑错误类型

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.3-UNIT-004 | Unit | P0 | Detect timeline contradictions | Core error type 1 |
| 1.3-UNIT-005 | Unit | P0 | Detect character inconsistencies | Core error type 2 |
| 1.3-UNIT-006 | Unit | P0 | Detect plot logic holes | Core error type 3 |
| 1.3-UNIT-007 | Unit | P0 | Detect dialogue coherence issues | Core error type 4 |
| 1.3-UNIT-008 | Unit | P0 | Detect scene transition problems | Core error type 5 |
| 1.3-INT-003 | Integration | P0 | Detect multiple error types in one script | Complex scenario |
| 1.3-INT-004 | Integration | P1 | Prioritize critical errors | Error ranking |

### AC3: 为每个检测到的问题提供具体位置和详细说明

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.3-UNIT-009 | Unit | P0 | Generate error location (line/scene) | Location tracking |
| 1.3-UNIT-010 | Unit | P0 | Generate detailed error description | User clarity |
| 1.3-UNIT-011 | Unit | P1 | Include context in error description | Enhanced UX |
| 1.3-INT-005 | Integration | P0 | Map AI response to script locations | Critical mapping |
| 1.3-INT-006 | Integration | P1 | Generate actionable suggestions | User value |

### AC4: 生成结构化的分析报告供后续处理

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.3-UNIT-012 | Unit | P0 | Generate valid JSON report structure | Output format |
| 1.3-UNIT-013 | Unit | P0 | Include all required report fields | Completeness |
| 1.3-UNIT-014 | Unit | P1 | Generate summary statistics | Overview data |
| 1.3-INT-007 | Integration | P0 | Export report in multiple formats | Flexibility |
| 1.3-E2E-002 | E2E | P1 | Report usable by downstream systems | Integration |

### AC5: 支持通过DeepSeek API进行AI推理

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.3-UNIT-015 | Unit | P0 | Build valid AI prompts | Prompt engineering |
| 1.3-UNIT-016 | Unit | P0 | Parse AI responses correctly | Response handling |
| 1.3-UNIT-017 | Unit | P1 | Handle AI response variations | Robustness |
| 1.3-INT-008 | Integration | P0 | Call DeepSeek API successfully | API integration |
| 1.3-INT-009 | Integration | P0 | Handle API errors gracefully | Error recovery |
| 1.3-INT-010 | Integration | P1 | Retry on transient failures | Resilience |
| 1.3-E2E-003 | E2E | P0 | Complete AI analysis cycle | Critical path |

### AC6: 分析响应时间符合性能要求（<10秒）

#### Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.3-UNIT-018 | Unit | P0 | Optimize prompt length | Performance |
| 1.3-UNIT-019 | Unit | P1 | Implement response caching | Optimization |
| 1.3-INT-011 | Integration | P0 | Measure API response time | Performance metric |
| 1.3-INT-012 | Integration | P0 | Handle timeout gracefully | Failure handling |
| 1.3-E2E-004 | E2E | P0 | End-to-end under 10 seconds | SLA requirement |
| 1.3-E2E-005 | E2E | P1 | Performance under load | Scalability |

## AI-Specific Test Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.3-AI-001 | Integration | P0 | Validate prompt injection prevention | Security |
| 1.3-AI-002 | Integration | P0 | Test response consistency | Reliability |
| 1.3-AI-003 | Integration | P1 | Measure false positive rate | Quality |
| 1.3-AI-004 | Integration | P1 | Measure false negative rate | Quality |
| 1.3-AI-005 | Unit | P1 | Test prompt template generation | Correctness |
| 1.3-AI-006 | Unit | P2 | Test dynamic prompt building | Flexibility |
| 1.3-AI-007 | Integration | P2 | Test with different AI models | Future-proofing |

## Performance Test Scenarios

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 1.3-PERF-001 | E2E | P0 | 10-page script < 10s | Basic requirement |
| 1.3-PERF-002 | Integration | P1 | 50-page script handling | Large scripts |
| 1.3-PERF-003 | Integration | P1 | Concurrent analysis requests | Scalability |
| 1.3-PERF-004 | Unit | P2 | Memory usage optimization | Resource efficiency |
| 1.3-PERF-005 | Integration | P2 | Cache hit rate > 30% | Optimization |

## Risk Coverage Matrix

Mapping test scenarios to identified risks from risk profile:

| Risk ID | Risk Title | Mitigating Test Scenarios |
|---------|------------|--------------------------|
| PERF-001 | Response time >10s | 1.3-PERF-001, 1.3-INT-011, 1.3-E2E-004 |
| TECH-001 | AI unpredictability | 1.3-AI-002, 1.3-UNIT-016, 1.3-UNIT-017 |
| DATA-001 | Context loss | 1.3-PERF-002, 1.3-INT-003 |
| BUS-001 | False positives | 1.3-AI-003, 1.3-AI-004 |

## Test Data Requirements

### Sample Scripts with Known Errors

#### Timeline Contradiction Script
```typescript
const timelineErrorScript = {
  scenes: [
    { title: "Morning", time: "8:00 AM", description: "John wakes up" },
    { title: "Later", time: "7:30 AM", description: "John arrives at work" }
  ]
}
```

#### Character Inconsistency Script
```typescript
const characterErrorScript = {
  scenes: [
    { dialogue: { character: "Mary", content: "I've never been to Paris" }},
    { dialogue: { character: "Mary", content: "When I lived in Paris last year..." }}
  ]
}
```

#### Clean Script (No Errors)
```typescript
const cleanScript = {
  scenes: [...],  // Well-structured, consistent script
  expectedErrors: []
}
```

### AI Mock Responses

#### Successful Detection Response
```json
{
  "errors": [
    {
      "type": "timeline_contradiction",
      "location": { "scene": 2, "line": 15 },
      "description": "Event occurs before character arrives",
      "severity": "high",
      "suggestion": "Reorder scenes or adjust timestamps"
    }
  ],
  "summary": {
    "total_errors": 1,
    "critical": 1
  }
}
```

#### Edge Cases
- Scripts with 100+ scenes
- Scripts with minimal dialogue
- Scripts in mixed languages
- Scripts with complex nested timelines
- Scripts with ambiguous character names

### Performance Test Data
- 10-page script (baseline)
- 50-page script (stress test)
- 100-page script (boundary test)
- Concurrent requests (5, 10, 20)

## Recommended Execution Order

### Phase 1: Core Functionality (P0 Tests)
1. **Unit Tests** (Foundation)
   - Prompt building (1.3-UNIT-015)
   - Error detection logic (1.3-UNIT-004 through 008)
   - Response parsing (1.3-UNIT-016)
   - Report generation (1.3-UNIT-012)

2. **Integration Tests** (API Integration)
   - DeepSeek API calls (1.3-INT-008)
   - Error handling (1.3-INT-009)
   - Response mapping (1.3-INT-005)

3. **E2E Tests** (Critical Paths)
   - Complete analysis (1.3-E2E-003)
   - Performance SLA (1.3-E2E-004)

### Phase 2: Quality & Reliability (P1 Tests)
- AI consistency tests
- False positive/negative measurement
- Error context and suggestions
- Large script handling

### Phase 3: Optimization (P2 Tests)
- Caching effectiveness
- Resource optimization
- Alternative AI models

## Test Automation Strategy

### Unit Tests (Jest)
```typescript
describe('ConsistencyGuardian', () => {
  describe('Prompt Engineering', () => {
    // 1.3-UNIT-015 through 1.3-UNIT-017
  });
  
  describe('Error Detection', () => {
    // 1.3-UNIT-004 through 1.3-UNIT-008
  });
  
  describe('Report Generation', () => {
    // 1.3-UNIT-012 through 1.3-UNIT-014
  });
});
```

### Integration Tests (Jest + MSW)
```typescript
describe('AI Integration', () => {
  describe('DeepSeek API', () => {
    // Mock AI responses
    // 1.3-INT-008 through 1.3-INT-010
  });
  
  describe('Response Processing', () => {
    // 1.3-INT-005, 1.3-INT-006
  });
});
```

### E2E Tests (Playwright)
```typescript
describe('Consistency Analysis E2E', () => {
  // 1.3-E2E-001 through 1.3-E2E-005
  // Performance benchmarks
});
```

## Quality Checklist

- ✅ Every AC has test coverage
- ✅ Test levels appropriate (52% unit, 33% integration, 15% E2E)
- ✅ No duplicate coverage across levels
- ✅ AI-specific risks addressed
- ✅ Performance requirements validated
- ✅ Test IDs follow naming convention
- ✅ Scenarios are atomic and independent

## Gate YAML Block

```yaml
test_design:
  scenarios_total: 42
  by_level:
    unit: 22
    integration: 14
    e2e: 6
  by_priority:
    p0: 18
    p1: 15
    p2: 9
  ai_specific_tests: 7
  performance_tests: 5
  coverage_gaps: []
  risk_coverage:
    critical_risks: N/A
    high_risks: 100%
```

## Summary

This test design provides comprehensive coverage for the Consistency Guardian Agent with:
- Strong focus on AI prompt engineering and response validation
- Extensive error detection coverage for all 5 core types
- Critical performance validation for <10s requirement
- AI-specific tests for reliability and accuracy
- Clear execution priorities based on risk

The design addresses the unique challenges of AI integration including response unpredictability, false positive management, and performance optimization.

Test design matrix location: `docs/qa/assessments/1.3-test-design-20250831.md`
P0 tests identified: 18